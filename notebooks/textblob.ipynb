{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/victor/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install the requirements\n",
    "#!pip install -U textblob\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# `textblob`: otro módulo para tareas de PLN (`NLTK` + `pattern`)\n",
    "\n",
    "[textblob](http://textblob.readthedocs.org/) es una librería de procesamiento del texto para Python que permite realizar tareas de Procesamiento del Lenguaje Natural como análisis morfológico, extracción de entidades, análisis de opinión, traducción automática, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Está construida sobre otras dos librerías muy famosas de Python: [NLTK](http://www.nltk.org/) y [pattern](http://www.clips.ua.ac.be/pages/pattern-en). La principal ventaja de [textblob](http://textblob.readthedocs.org/) es que permite combinar el uso de las dos herramientas anteriores en un interfaz más simple.\n",
    "\n",
    "Vamos a apoyarnos en [este tutorial](http://textblob.readthedocs.org/en/dev/quickstart.html) para aprender a utilizar algunas de sus funcionalidades más llamativas. \n",
    "\n",
    "Lo primero es importar el objeto `TextBlob` que nos permite acceder a todas las herramentas que incluye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Vamos a crear nuestro primer ejemplo de *textblob* a través del objeto `TextBlob`. Piensa en estos *textblobs* como una especie de cadenas de texto de Python, analaizadas y enriquecidas con algunas características extra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "texto = \"\"\"In new lawsuits brought against the ride-sharing companies Uber and Lyft, the top prosecutors in Los Angeles \n",
    "and San Francisco counties make an important point about the lightly regulated sharing economy. The consumers who \n",
    "participate deserve a very clear picture of the risks they're taking.\"\"\"\n",
    "t = TextBlob(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"In new lawsuits brought against the ride-sharing companies Uber and Lyft, the top prosecutors in Los Angeles \n",
      "and San Francisco counties make an important point about the lightly regulated sharing economy.\"), Sentence(\"The consumers who \n",
      "participate deserve a very clear picture of the risks they're taking.\")]\n"
     ]
    }
   ],
   "source": [
    "print(t.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 2 oraciones.\n",
      "\n",
      "In new lawsuits brought against the ride-sharing companies Uber and Lyft, the top prosecutors in Los Angeles \n",
      "and San Francisco counties make an important point about the lightly regulated sharing economy.\n",
      "---------------------------------------------------------------------------\n",
      "The consumers who \n",
      "participate deserve a very clear picture of the risks they're taking.\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Tenemos\", len(t.sentences), \"oraciones.\\n\")\n",
    "\n",
    "for sentence in t.sentences:\n",
    "    print(sentence)\n",
    "    print(\"-\" * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Procesando oraciones, palabras y entidades\n",
    "\n",
    "Podemos segmentar en oraciones y en palabras nuestra texto de ejemplo simplemente accediendo a las propiedades `.sentences` y `.words`. Imprimimos por pantalla: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In new lawsuits brought against the ride-sharing companies Uber and Lyft, the top prosecutors in Los Angeles \n",
      "and San Francisco counties make an important point about the lightly regulated sharing economy.\n",
      "---------------------------------------------------------------------------\n",
      "The consumers who \n",
      "participate deserve a very clear picture of the risks they're taking.\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# imprimimos las oraciones\n",
    "for sentence in t.sentences:\n",
    "    print(sentence)\n",
    "    print(\"-\" * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'new', 'lawsuits', 'brought', 'against', 'the', 'ride-sharing', 'companies', 'Uber', 'and', 'Lyft', 'the', 'top', 'prosecutors', 'in', 'Los', 'Angeles', 'and', 'San', 'Francisco', 'counties', 'make', 'an', 'important', 'point', 'about', 'the', 'lightly', 'regulated', 'sharing', 'economy', 'The', 'consumers', 'who', 'participate', 'deserve', 'a', 'very', 'clear', 'picture', 'of', 'the', 'risks', 'they', \"'re\", 'taking']\n",
      "['In', 'new', 'lawsuits', 'brought', 'against', 'the', 'ride-sharing', 'companies', 'Uber', 'and', 'Lyft,', 'the', 'top', 'prosecutors', 'in', 'Los', 'Angeles', 'and', 'San', 'Francisco', 'counties', 'make', 'an', 'important', 'point', 'about', 'the', 'lightly', 'regulated', 'sharing', 'economy.', 'The', 'consumers', 'who', 'participate', 'deserve', 'a', 'very', 'clear', 'picture', 'of', 'the', 'risks', \"they're\", 'taking.']\n"
     ]
    }
   ],
   "source": [
    "# y las palabras\n",
    "print(t.words)\n",
    "print(texto.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "La propiedad `.noun_phrases` nos permite acceder a la lista de entidades (en realidad, son sintagmas nominales) incluídos en nuestro *textblob*. Así es como funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"el texto de ejemplo contiene\", len(t.noun_phrases), \"entidades\")\n",
    "for element in t.noun_phrases:\n",
    "    print(\"-\", element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# jugando con lemas, singulares y plurales\n",
    "for word in t.words:\n",
    "    if word.endswith(\"s\"):\n",
    "        print(word.lemmatize(), word, word.singularize())\n",
    "    else:\n",
    "        print(word.lemmatize(), word, word.pluralize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# ¿cómo podemos hacer la lematización más inteligente?\n",
    "for item in t.tags:\n",
    "    if item[1] == \"NN\":\n",
    "        print(item[0], \"-->\", item[0].pluralize())\n",
    "    elif item[1] == \"NNS\":\n",
    "        print(item[0], \"-->\", item[0].singularize())\n",
    "    else:\n",
    "        print(item[0], item[0].lemmatize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Análisis sintático\n",
    "\n",
    "Aunque podemos utilizar otros analizadores, por defecto el método `.parse()` invoca al analizador morfosintáctico del módulo  `pattern.en` que ya conoces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# análisis sintáctico\n",
    "print(t.parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Traducción automática\n",
    "\n",
    "\n",
    "A partir de cualquier texto procesado con `TextBlob`, podemos acceder a un traductor automático de bastante calidad con el método `.translate`. Fíjate en cómo lo usamos. Es obligatorio indicar la lengua de destinto. La lengua de origen, se puede predecir a partir del texto de entrada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Chinese lunar exploration project, also known as the Chang'e Project, is the first lunar exploration project launched in China and was officially launched on March 1, 2003\n",
      "El proyecto de exploración lunar chino, también conocido como el Proyecto Chang'e, es el primer proyecto de exploración lunar lanzado en China y se lanzó oficialmente el 1 de marzo de 2003.\n",
      "In 1943 she was sent to the United States, where she defended the British White Paper, after which she worked in Canada and India.\n",
      "En 1943 fue enviada a los Estados Unidos, donde defendió el Libro Blanco británico, después de lo cual trabajó en Canadá e India.\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# de chino a inglés y español\n",
    "oracion_zh = \"中国探月工程 亦稱嫦娥工程，是中国启动的第一个探月工程，于2003年3月1日正式启动\"\n",
    "t_zh = TextBlob(oracion_zh)\n",
    "print(t_zh.translate(from_lang=\"zh-CN\", to=\"en\"))\n",
    "print(t_zh.translate(from_lang=\"zh-CN\", to=\"es\"))\n",
    "\n",
    "oracion_ru = \"В 1943 году была отправлена в США, где выступала в защиту британской «белой книги», после чего работала в Канаде и Индии.\"\n",
    "t_ru = TextBlob(oracion_ru)\n",
    "print(t_ru.translate(from_lang=\"ru\", to=\"en\"))\n",
    "print(t_ru.translate(from_lang=\"ru\", to=\"es\"))\n",
    "\n",
    "print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al contenido aquí le faltan citas. Por favor, cite fuentes confiables\n",
      ". Cualquier información no autenticada puede ser puesta en duda y eliminada. (Diciembre 2018)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    TextBlob(\n",
    "        \"\"\"المحتوى هنا ينقصه الاستشهاد بمصادر. يرجى إيراد مصادر موثوق بها\n",
    ". أي معلومات غير موثقة يمكن التشكيك بها وإزالتها. (ديسمبر 2018)\"\"\"\n",
    "    ).translate(to=\"es\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το δημόσιο χρέος έχει δημιουργήσει νέα ρεκόρ στην Ισπανία το τρίτο τρίμηνο\n",
      "Государственный долг установил новые рекорды в Испании в третьем квартале\n",
      "Zor publikoak errekor berriak ezarri ditu Espainian hirugarren hiruhilekoan\n",
      "Julkinen velka on asettanut uusia ennätyksiä Espanjassa kolmannella vuosineljänneksellä\n",
      "La dette publique a établi de nouveaux records en Espagne au troisième trimestre\n",
      "De overheidsschuld heeft in het derde kwartaal een nieuw record gevestigd in Spanje\n",
      "A débeda pública estableceu novos rexistros en España no terceiro trimestre\n",
      "El deute públic ha marcat nous rècords a Espanya en el tercer trimestre\n",
      "第三季度公共债务在西班牙创下新纪录\n",
      "Debitum palam novum records in Hispaniam profectus est, in tertia quartam\n",
      "Veřejný dluh vytvořil ve Španělsku nové záznamy ve třetím čtvrtletí\n"
     ]
    }
   ],
   "source": [
    "t_es = TextBlob(\n",
    "    \"La deuda pública ha marcado nuevos récords en España en el tercer trimestre\"\n",
    ")\n",
    "print(t_es.translate(to=\"el\"))\n",
    "print(t_es.translate(to=\"ru\"))\n",
    "print(t_es.translate(to=\"eu\"))\n",
    "print(t_es.translate(to=\"fi\"))\n",
    "print(t_es.translate(to=\"fr\"))\n",
    "print(t_es.translate(to=\"nl\"))\n",
    "print(t_es.translate(to=\"gl\"))\n",
    "print(t_es.translate(to=\"ca\"))\n",
    "print(t_es.translate(to=\"zh\"))\n",
    "print(t_es.translate(to=\"la\"))\n",
    "print(t_es.translate(to=\"cs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "I went to Milan and enjoyed a brothel.\n",
      "Fui a Milán y disfruté de un burdel.\n"
     ]
    }
   ],
   "source": [
    "# con el slang no funciona tan bien\n",
    "print(\"--------------\")\n",
    "t_ita = TextBlob(\"Sono andato a Milano e mi sono divertito un bordello.\")\n",
    "print(t_ita.translate(to=\"en\"))\n",
    "print(t_ita.translate(to=\"es\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## WordNet\n",
    "\n",
    "`textblob`, más concretamente, cualquier objeto de la clase `Word`, nos permite acceder a la información de WordNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# WordNet\n",
    "from textblob import Word\n",
    "from textblob.wordnet import VERB\n",
    "\n",
    "# ¿cuántos synsets tiene \"car\"\n",
    "word = Word(\"car\")\n",
    "print(word.synsets)\n",
    "\n",
    "# dame los synsets de la palabra \"hack\" como verbo\n",
    "print(Word(\"hack\").get_synsets(pos=VERB))\n",
    "\n",
    "# imprime la lista de definiciones de \"car\"\n",
    "print(Word(\"car\").definitions)\n",
    "\n",
    "# recorre la jerarquía de hiperónimos\n",
    "for s in word.synsets:\n",
    "    print(s.hypernym_paths())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Análisis de opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.4683712121212122, subjectivity=0.4681818181818182)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "0.4683712121212122\n"
     ]
    }
   ],
   "source": [
    "# análisis de opinión\n",
    "opinion1 = TextBlob(\"This new restaurant is great. I had so much fun!!\")\n",
    "print(opinion1.sentiment)\n",
    "\n",
    "opinion2 = TextBlob(\"Google News to close in Spain.\")\n",
    "print(opinion2.sentiment)\n",
    "\n",
    "# subjetividad 0:1\n",
    "# polaridad -1:1\n",
    "\n",
    "print(opinion1.sentiment.polarity)\n",
    "\n",
    "if opinion1.sentiment.subjectivity > 0.5:\n",
    "    print(\"Hey, esto es una opinion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Ejercicio 1\n",
    "\n",
    "Prueba a analizar distintas oraciones en inglés (combinando verbos que indican información subjetiva, palabras con distinta carga emocional, añadiendo emoticonos, etc.) para ver si eres capaz de entender el funcionamiento de este analizador de opiniones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase I enjoy football. tiene subjetividad 0.5 \n",
      "          y polaridad 0.4\n",
      "La frase I hate tennis. tiene subjetividad 0.9 \n",
      "          y polaridad -0.8\n",
      "La frase I really hate tennis. tiene subjetividad 0.9 \n",
      "          y polaridad -0.8\n",
      "La frase I hate tennis so much. tiene subjetividad 0.55 \n",
      "          y polaridad -0.30000000000000004\n",
      "La frase I HATE tennis. tiene subjetividad 0.9 \n",
      "          y polaridad -0.8\n",
      "La frase I HATE tennis ABOVE ALL THINGS!! tiene subjetividad 0.5 \n",
      "          y polaridad -0.4\n",
      "La frase I fucking hate tennis :-( tiene subjetividad 0.95 \n",
      "          y polaridad -0.775\n",
      "La frase This place is terrible tiene subjetividad 1.0 \n",
      "          y polaridad -1.0\n"
     ]
    }
   ],
   "source": [
    "# escribe tu código aquí\n",
    "frases = [\n",
    "    \"I enjoy football.\",\n",
    "    \"I hate tennis.\",\n",
    "    \"I really hate tennis.\",\n",
    "    \"I hate tennis so much.\",\n",
    "    \"I HATE tennis.\",\n",
    "    \"I HATE tennis ABOVE ALL THINGS!!\",\n",
    "    \"I fucking hate tennis :-(\",\n",
    "    \"This place is terrible\",\n",
    "]\n",
    "\n",
    "for frase in frases:\n",
    "    t = TextBlob(frase)\n",
    "    print(f\"\"\"La frase {frase} tiene subjetividad {t.sentiment.subjectivity} \n",
    "          y polaridad {t.sentiment.polarity}\"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "`TextBlob` da acceso a [otro tipo de analizadores](https://textblob.readthedocs.io/en/dev/advanced_usage.html#sentiment-analyzers) de opinión, por ejemplo, un clasificador basado en *Naive Bayes*. Prueba qué tal funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy football.\n",
      "subj: Sentiment(classification='neg', p_pos=0.4551209103840678, p_neg=0.5448790896159322)\n",
      "\n",
      "I hate tennis.\n",
      "subj: Sentiment(classification='pos', p_pos=0.7669683257918551, p_neg=0.2330316742081448)\n",
      "\n",
      "I really hate tennis.\n",
      "subj: Sentiment(classification='pos', p_pos=0.7649028958679761, p_neg=0.23509710413202378)\n",
      "\n",
      "I hate tennis so much.\n",
      "subj: Sentiment(classification='pos', p_pos=0.7700920130170541, p_neg=0.2299079869829455)\n",
      "\n",
      "I HATE tennis.\n",
      "subj: Sentiment(classification='pos', p_pos=0.7669683257918551, p_neg=0.2330316742081448)\n",
      "\n",
      "I HATE tennis ABOVE ALL THINGS!!\n",
      "subj: Sentiment(classification='pos', p_pos=0.8414015521435758, p_neg=0.15859844785642296)\n",
      "\n",
      "I fucking hate tennis :-(\n",
      "subj: Sentiment(classification='pos', p_pos=0.6638381201044402, p_neg=0.33616187989556084)\n",
      "\n",
      "This place is terrible\n",
      "subj: Sentiment(classification='neg', p_pos=0.2607381165846103, p_neg=0.7392618834153899)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "for sentence in frases:\n",
    "    t = TextBlob(sentence, analyzer=NaiveBayesAnalyzer())\n",
    "    print(f\"{sentence}\\nsubj: {t.sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really enjoyed the acting. Anne Smith is one if my favorite actresses.\n",
      "subj: Sentiment(classification='pos', p_pos=0.7132001060897667, p_neg=0.2867998939102322)\n",
      "\n",
      "I left the movie theater right after the first 5 minutes. What a waste of time.\n",
      "subj: Sentiment(classification='neg', p_pos=0.17434761447801117, p_neg=0.8256523855219896)\n",
      "\n",
      "I love ice-cream\n",
      "subj: Sentiment(classification='pos', p_pos=0.5441400304414004, p_neg=0.45585996955859964)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I really enjoyed the acting. Anne Smith is one if my favorite actresses.\",\n",
    "    \"I left the movie theater right after the first 5 minutes. What a waste of time.\",\n",
    "    \"I love ice-cream\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    t = TextBlob(sentence, analyzer=NaiveBayesAnalyzer())\n",
    "    print(f\"{sentence}\\nsubj: {t.sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Otras curiosidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#  corrección ortográfica\n",
    "b1 = TextBlob(\"I havv goood speling!\")\n",
    "print(b1.correct())\n",
    "\n",
    "b2 = TextBlob(\"Miy naem iz Jonh!\")\n",
    "print(b2.correct())\n",
    "\n",
    "b3 = TextBlob(\"Boyz dont cri\")\n",
    "print(b3.correct())\n",
    "\n",
    "b4 = TextBlob(\"psicological posesion achifmen comitment\")\n",
    "print(b4.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Hasta el infinito, y más allá\n",
    "\n",
    "En este breve resumen solo consideramos las posibilidades que ofrece `TextBlob` por defecto. Pero si necesitas personalizar las herramientas, echa un vistazo a [la documentación avanzada](http://textblob.readthedocs.org/en/dev/advanced_usage.html#advanced). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
